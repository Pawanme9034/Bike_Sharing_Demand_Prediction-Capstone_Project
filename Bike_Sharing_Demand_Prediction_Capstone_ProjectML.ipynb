{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "T5CmagL3EC8N",
        "TIqpNgepFxVj",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pawanme9034/Bike_Sharing_Demand_Prediction-Capstone_Project/blob/main/Bike_Sharing_Demand_Prediction_Capstone_ProjectML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Bike Sharing Demand Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name-**  Pawan Kumar Singh\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Pawanme9034/Bike_Sharing_Demand_Prediction-Capstone_Project/blob/main/Bike_Sharing_Demand_Prediction_Capstone_ProjectML.ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Statement is the prediction of bike count required at each hour for the stable supply of rental bikes.\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dByMsuzT8Tnw"
      },
      "source": [
        "#let's import the modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "import requests\n",
        "from io import StringIO\n",
        "# uploading data through Github directly\n",
        "url = \"https://raw.githubusercontent.com/Pawanme9034/Bike_Sharing_Demand_Prediction-Capstone_Project/main/SeoulBikeData.csv\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:66.0) Gecko/20100101 Firefox/66.0\"}\n",
        "req = requests.get(url, headers=headers)\n",
        "data = StringIO(req.text)\n",
        "\n",
        "bike_df=pd.read_csv(data)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "bike_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "bike_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "value=len(bike_df[bike_df.duplicated()])\n",
        "value"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "bike_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. This Dataset contains 8760 lines and 14 columns.\n",
        "2. the data is orgenized and there are timestamp.\n",
        "3. there are no missing or null values in dataset.\n",
        "4. dtypes: float64(6), int64(4), object(4)\n",
        "5. memory usage: 848.3+ KB\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "bike_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "bike_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Date** : *The date of the day, during 365 days from 01/12/2017 to 30/11/2018, formating in DD/MM/YYYY, type : str*, we need to convert into datetime format.\n",
        "\n",
        "**Rented Bike Count** : *Number of rented bikes per hour which our dependent variable and we need to predict that, type : int*\n",
        "\n",
        "**Hour**: *The hour of the day, starting from 0-23 it's in a digital time format, type : int, we need to convert it into category data type.*\n",
        "\n",
        "**Temperature(°C)**: *Temperature in Celsius, type : Float*\n",
        "\n",
        "**Humidity(%)**: *Humidity in the air in %, type : int*\n",
        "\n",
        "**Wind speed (m/s)** : *Speed of the wind in m/s, type : Float*\n",
        "\n",
        "**Visibility (10m)**: *Visibility in m, type : int*\n",
        "\n",
        "**Dew point temperature(°C)**: *Temperature at the beggining of the day, type : Float*\n",
        "\n",
        "**Solar Radiation (MJ/m2)**: *Sun contribution, type : Float*\n",
        "\n",
        "**Rainfall(mm)**: *Amount of raining in mm, type : Float*\n",
        "\n",
        "**Snowfall (cm)**: *Amount of snowing in cm, type : Float*\n",
        "\n",
        "**Seasons**: *Season of the year, type : str, there are only 4 season's in data *.\n",
        "\n",
        "**Holiday**: *If the day  is holiday period or not, type: str*\n",
        "\n",
        "**Functioning Day**: *If the day is a Functioning Day or not, type : str* *italicized text* **bold text**\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(bike_df.apply(lambda col: col.unique()))"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the unique value\n",
        "bike_df.nunique()"
      ],
      "metadata": {
        "id": "i4mWRdXxa2V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "#Rename the complex columns name\n",
        "bike_df=bike_df.rename(columns={'Rented Bike Count':'Rented_Bike_Count',\n",
        "                                'Temperature(°C)':'Temperature',\n",
        "                                'Temperature(�C)':'Temperature',\n",
        "                                'Humidity(%)':'Humidity',\n",
        "                                'Wind speed (m/s)':'Wind_speed',\n",
        "                                'Visibility (10m)':'Visibility',\n",
        "                                'Dew point temperature(°C)':'Dew_point_temperature',\n",
        "                                'Dew point temperature(�C)':'Dew_point_temperature',\n",
        "                                'Solar Radiation (MJ/m2)':'Solar_Radiation',\n",
        "                                'Rainfall(mm)':'Rainfall',\n",
        "                                'Snowfall (cm)':'Snowfall',\n",
        "                                'Functioning Day':'Functioning_Day'})"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking columns names\n",
        "bike_df.columns"
      ],
      "metadata": {
        "id": "BXmGErKVQP-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the \"Date\" column into three \"year\",\"month\",\"day\" column\n",
        "# Convert 'Date' column to datetime format\n",
        "bike_df['Date'] = bike_df['Date'].apply(lambda x:dt.datetime.strptime(x,\"%d/%m/%Y\"))"
      ],
      "metadata": {
        "id": "vNKBcgGjS-OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract year, month, and day of the week\n",
        "bike_df['year'] = bike_df['Date'].dt.year\n",
        "bike_df['month'] = bike_df['Date'].dt.month\n",
        "bike_df['day'] = bike_df['Date'].dt.day_name()"
      ],
      "metadata": {
        "id": "eFbxRfToVKUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n",
        "bike_df['weekdays_weekend']=bike_df['day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )\n",
        "# Drop 'Date', 'day', and 'year' columns\n",
        "bike_df=bike_df.drop(columns=['Date','day','year'],axis=1)"
      ],
      "metadata": {
        "id": "FGqDhH03VPKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get value counts of 'weekdays_weekend' column\n",
        "bike_df['weekdays_weekend'].value_counts()"
      ],
      "metadata": {
        "id": "d45w6rYBYjQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So we convert the \"date\" column into 3 different column i.e \"year\",\"month\",\"day\"\n",
        "* The \"year\" column in our data set is basically contain the 2 unique number contains the details of from 2017 december to 2018 november so if i consider this is a one year then we don't need the \"year\" column so we drop it.\n",
        "* The other column \"day\", it contains the details about the each day of the month, for our relevence we don't need each day of each month data but we need the data about, if a day is a weekday or a weekend so we convert it into this format and drop the \"day\" column"
      ],
      "metadata": {
        "id": "ryZ5NezGVVhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the int64 column into catagory column\n",
        "cols=['Hour','month','weekdays_weekend']\n",
        "for col in cols:\n",
        "  bike_df[col]=bike_df[col].astype('category')"
      ],
      "metadata": {
        "id": "Wlb1NivUWc12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign the numerical columns of the DataFrame 'bike_df' to a variable,\n",
        "# Select numerical columns\n",
        "numerical_columns=list(bike_df.select_dtypes(['int64','float64']).columns)\n",
        "numerical_features=pd.Index(numerical_columns)\n",
        "numerical_features"
      ],
      "metadata": {
        "id": "vYNveBExrwjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Rename the complex columns name\n",
        "2. Changing the \"Date\" column into three \"year\",\"month\",\"day\" column\n",
        "3. Convert 'Date' column to datetime format\n",
        "4. Extract year, month, and day of the week\n",
        "5. creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n",
        "6. Drop 'Date', 'day', and 'year' columns\n",
        "7. Change the int64 column into catagory column\n",
        "8. assign the numerical columns of the DataFrame 'bike_df' to a variable,\n",
        "\n",
        "Sure! Here's a summary of the manipulation code you provided previously:\n",
        "\n",
        "1. Extracting Year, Month, and Day:\n",
        "   - You added three columns ('year', 'month', 'day') to the DataFrame 'bike_df'.\n",
        "   - The 'year' column contains the year extracted from the 'Date' column using the 'dt.year' property.\n",
        "   - The 'month' column contains the month extracted from the 'Date' column using the 'dt.month' property.\n",
        "   - The 'day' column contains the day of the week (e.g., 'Sunday', 'Monday') extracted from the 'Date' column using the 'dt.day_name()' method.\n",
        "\n",
        "2. Converting Date column to datetime format:\n",
        "   - You used the 'apply()' function with a lambda function to convert the 'Date' column to a datetime format.\n",
        "   - The lambda function utilized 'strptime()' from the 'datetime' module to parse each date string according to the format \"%d/%m/%Y\".\n",
        "   - The resulting datetime objects were assigned back to the 'Date' column.\n",
        "\n",
        "3. Determining Weekdays vs. Weekends:\n",
        "   - You added a new column called 'weekdays_weekend' to the DataFrame 'bike_df'.\n",
        "   - The values in the 'weekdays_weekend' column were determined using the 'apply()' function and a lambda function.\n",
        "   - If the 'day' value was 'Saturday' or 'Sunday', the corresponding value in the 'weekdays_weekend' column was set to 1; otherwise, it was set to 0.\n",
        "\n",
        "4. Dropping Columns:\n",
        "   - You dropped the 'Date', 'day', and 'year' columns from the DataFrame 'bike_df' using the 'drop()' function.\n",
        "   - The 'columns' parameter was used to specify the names of the columns to be dropped, and the 'axis' parameter was set to 1 to indicate column-wise operation.\n",
        "\n",
        "5. Renaming Complex Column Names:\n",
        "   - You renamed the complex column names in the DataFrame 'bike_df' to make them more analysis-ready.\n",
        "   - The 'rename()' function was used with a dictionary mapping the current column names to the desired new column names.\n",
        "   - The column names that were renamed included 'Rented Bike Count', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)', and 'Functioning Day'.\n",
        "\n",
        "These manipulations demonstrate various data preprocessing steps, such as extracting date components, converting data types, creating derived features, and renaming columns."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rented bikes according to the month"
      ],
      "metadata": {
        "id": "Y2fQIv7abr57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "sns.barplot(data=bike_df, x='month', y='Rented_Bike_Count', ax=ax, capsize=.2, hue='Seasons')\n",
        "ax.set_title('Count of Rented Bikes According to Month')\n",
        "ax.set_xlabel('Month')\n",
        "ax.set_ylabel('Rented Bike Count')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FvUobJ7hhah2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar plot was chosen because it allows for a clear comparison of the count of rented bikes across different months. The categorical nature of the months is well-suited for representation on the x-axis, while the numerical count of rented bikes is represented on the y-axis. The use of different colors for each season aids in visual comparison. The bar plot enables the identification of any seasonal patterns or variations in bike rental demand and facilitates easy interpretation of the data. Overall, it is an effective and concise way to convey the count of rented bikes for each month and analyze trends over time."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seasonal Variations: The average count of rented bikes varies across seasons. Summer months (June, July, and August) have the highest average count, while winter months have relatively lower counts.\n",
        "\n",
        "Monthly Variations: Within each season, there are variations in the average count of rented bikes across different months.\n",
        "\n",
        "Winter Season: The winter season generally exhibits lower average counts of rented bikes, suggesting reduced demand during colder months."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can positively impact the business by informing decisions related to seasonal demand planning, marketing strategies, and pricing. Understanding the seasonal and monthly variations in bike rental demand allows businesses to optimize resource allocation, target promotions effectively, and adjust pricing strategies. However, the lower average counts during the winter season may pose a temporary negative impact. Nonetheless, this insight can also present opportunities for diversification or focusing on alternative revenue streams during colder months. It is crucial for businesses to analyze these insights in their specific context to make informed decisions and drive positive business outcomes."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hourly Distribution"
      ],
      "metadata": {
        "id": "RDlem-0-U4pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "sns.pointplot(data=bike_df, x='Hour', y='Rented_Bike_Count', hue='weekdays_weekend', ax=ax, markers=[\"o\", \"s\"], linestyles=[\"-\", \"--\"], palette=[\"#1f77b4\", \"#ff7f0e\"])\n",
        "ax.set(title='Count of Rented Bikes According to Weekdays/Weekends', xlabel='Hour', ylabel='Rented Bike Count')\n",
        "\n",
        "# Calculating the percentage of weekdays and weekends\n",
        "weekday_percentage = bike_df['weekdays_weekend'].value_counts(normalize=True) * 100\n",
        "weekday_percentage = weekday_percentage.round(2)\n",
        "\n",
        "# Adding percentage labels\n",
        "for patch in ax.patches:\n",
        "    height = patch.get_height()\n",
        "    ax.text(patch.get_x() + patch.get_width() / 2, height + 5, f'{height:.2f}%', ha='center')\n",
        "\n",
        "# Adding legend with percentage labels\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "labels = [f'{label} ({weekday_percentage[i]}%)' for i, label in enumerate(labels)]\n",
        "ax.legend(handles, labels)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VjMPxRzFooJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart, a point plot with hue grouping, was chosen to visualize the count of rented bikes according to weekdays and weekends on an hourly basis. It allows for easy comparison between weekdays and weekends and provides a clear understanding of the distribution of bike counts throughout the day. The inclusion of percentage labels adds further context and insights to the chart. Overall, this chart effectively presents the desired information in a concise and visually appealing manner."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from the chart indicate that bike rental counts vary throughout the day. Weekdays show higher rental counts during peak commuting hours, while weekends have higher counts in the afternoon and early evening. The highest average counts are observed on weekday evenings and weekend afternoons. These insights suggest different usage patterns between weekdays and weekends and can inform resource planning and marketing strategies."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The insights gained from the data can have a positive impact on the business by optimizing operations and resource allocation. Understanding peak demand periods allows businesses to improve customer satisfaction and revenue. However, insights showing consistently low rental counts during certain hours may indicate a need to attract customers during off-peak times to avoid negative growth. Overall, leveraging these insights helps businesses make informed decisions to drive positive business outcomes."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count of Rented Bikes According to Seasons"
      ],
      "metadata": {
        "id": "VWKYEKdXvOST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "sns.boxplot(data=bike_df, x='Seasons', y='Rented_Bike_Count', ax=ax)\n",
        "ax.set(title='Count of Rented Bikes According to Seasons', xlabel='Seasons', ylabel='Rented Bike Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BV67CPCjs7i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart, a boxplot, was chosen to visualize the count of rented bikes according to seasons because it effectively displays the distribution of the data and provides insights into the variability and central tendency of the rented bike counts across different seasons. The boxplot allows for easy comparison between seasons, showing the median, quartiles, and potential outliers. It also helps identify any seasonal patterns or differences in the rented bike counts. Overall, the boxplot is a suitable choice for understanding the distribution and variability of the data across different seasons."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from the chart are:\n",
        "\n",
        "1. There is variation in rented bike counts across different seasons.\n",
        "2. Summer has the highest bike rental demand.\n",
        "3. Winter has the lowest bike rental demand.\n",
        "\n",
        "These insights highlight the seasonal trends and can guide businesses in resource allocation and planning to meet customer demand effectively."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can help create a positive business impact. By understanding the seasonal variation in rented bike counts, businesses can strategically allocate resources and tailor their marketing efforts to capitalize on the peak demand during summer and other high-demand seasons. This can lead to increased customer satisfaction, improved operational efficiency, and potentially higher revenue.\n",
        "\n",
        "There are no insights from the chart that directly indicate negative growth. However, the lower bike rental demand during winter may pose a challenge for businesses during that season. They may need to implement alternative strategies such as offering discounts, promoting indoor activities, or diversifying their services to mitigate any potential negative impact on business growth."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rented Bike Count vs Snowfall"
      ],
      "metadata": {
        "id": "E0VQpXmBwhTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "fig, ax = plt.subplots(figsize=(18, 5))\n",
        "sns.scatterplot(data=bike_df, x='Snowfall', y='Rented_Bike_Count', ax=ax)\n",
        "ax.set(title='Rented Bike Count vs Snowfall', xlabel='Snowfall', ylabel='Rented Bike Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdTU_Zgxv2Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart, a scatter plot, was chosen to visualize the relationship between the \"Rented_Bike_Count\" and \"Snowfall\" variables. Scatter plots are effective in showing the distribution of data points and any potential patterns or trends between two numerical variables. In this case, the scatter plot allows us to examine how the rented bike count varies with different levels of snowfall. By plotting the \"Rented_Bike_Count\" on the y-axis and \"Snowfall\" on the x-axis, we can observe any potential correlation or relationship between these two variables."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight from the scatter plot chart is that there appears to be a mixed relationship between the \"Rented_Bike_Count\" and \"Snowfall\" variables.\n",
        "\n",
        "1. For low to moderate levels of snowfall (0-2.5), the rented bike count tends to be relatively high, indicating that people still use bikes for transportation despite the presence of snow.\n",
        "2. However, as the snowfall increases beyond 2.5, the rented bike count starts to decrease, suggesting that severe snowfall has a negative impact on bike usage.\n",
        "3. There are also some instances of higher rented bike counts at specific snowfall levels, such as at 0.6, 1.1, and 3.6, which may indicate unique factors or variations in customer behavior.\n",
        "\n",
        "Overall, the scatter plot highlights the complex relationship between snowfall and bike rentals, showing a mix of positive and negative impacts depending on the level of snowfall."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights from the scatter plot can potentially help create a positive business impact.\n",
        "\n",
        "Positive Impact:\n",
        "1. The insight that bike rentals remain relatively high during low to moderate levels of snowfall suggests that businesses can promote biking as a viable transportation option even in winter conditions. This can lead to increased revenue and customer engagement.\n",
        "\n",
        "Negative Impact:\n",
        "2. The insight that severe snowfall leads to a decrease in bike rentals indicates a potential negative impact on business growth during extreme weather conditions. In such situations, it may be challenging to attract customers and maintain regular bike rental activities.\n",
        "\n",
        "To mitigate the negative impact, businesses can focus on alternative services or promotions during periods of heavy snowfall, such as offering indoor cycling classes or maintenance services. By diversifying their offerings and adapting to changing weather conditions, businesses can minimize the negative effects and maintain a positive business impact overall."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the numerical data\n",
        "print(bike_df[['Temperature', 'Rented_Bike_Count']])\n"
      ],
      "metadata": {
        "id": "MJlQU5b9zoq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the style of the plot\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "# Create the scatter plot\n",
        "sns.scatterplot(data=bike_df, x='Temperature', y='Rented_Bike_Count', color='blue', alpha=0.5, marker='o', s=50)\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel('Temperature (°C)')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.title('Scatter Plot of Temperature vs Rented Bike Count')\n",
        "plt.legend(['Bike Data'], loc='upper right')\n",
        "\n",
        "# Adjust the plot aesthetics\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# Set the style of the plot\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "# Create the line plot\n",
        "sns.lineplot(data=bike_df, x='Solar_Radiation', y='Rented_Bike_Count', color='green')\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel('Solar Radiation')\n",
        "plt.ylabel('Mean Rented Bike Count')\n",
        "plt.title('Mean Rented Bike Count by Solar Radiation')\n",
        "\n",
        "# Adjust the plot aesthetics\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "bike_df.groupby('Wind_speed').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "num_plots = len(numerical_features)\n",
        "num_pairs = num_plots // 3  # Number of pairs of three\n",
        "remainder = num_plots % 3  # Remaining plots\n",
        "\n",
        "fig, axes = plt.subplots(num_pairs, 3, figsize=(21, num_pairs*3))\n",
        "\n",
        "for i in range(num_pairs):\n",
        "    for j in range(3):\n",
        "        col = numerical_features[i*3 + j]\n",
        "        sns.regplot(x=bike_df[col], y=bike_df['Rented_Bike_Count'], scatter_kws={\"color\": 'green'}, line_kws={\"color\": \"black\"}, ax=axes[i, j])\n",
        "        axes[i, j].set_xlabel(col)\n",
        "        axes[i, j].set_ylabel('Rented Bike Count')\n",
        "\n",
        "\n",
        "if remainder > 0:\n",
        "    for j in range(remainder):\n",
        "        col = numerical_features[num_pairs*3 + j]\n",
        "        sns.regplot(x=bike_df[col], y=bike_df['Rented_Bike_Count'], scatter_kws={\"color\": 'green'}, line_kws={\"color\": \"black\"}, ax=axes[num_pairs, j])\n",
        "        axes[num_pairs, j].set_xlabel(col)\n",
        "        axes[num_pairs, j].set_ylabel('Rented Bike Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CTK1jkzc8oH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "numerical_columns = [col for col in bike_df.columns if bike_df[col].dtype != object]\n",
        "num_plots = len(numerical_columns)\n",
        "num_pairs = num_plots // 6  # Number of pairs of six\n",
        "remainder = num_plots % 6  # Remaining plots\n",
        "\n",
        "fig, axes = plt.subplots(num_pairs, 6, figsize=(24, num_pairs*4))\n",
        "\n",
        "for i in range(num_pairs):\n",
        "    for j in range(6):\n",
        "        col = numerical_columns[i*6 + j]\n",
        "        sns.distplot(bike_df[col], ax=axes[i, j])\n",
        "        axes[i, j].set_xlabel(col)\n",
        "        axes[i, j].set_ylabel('Density')\n",
        "        axes[i, j].set_title(f'Distribution Plot: {col}')\n",
        "\n",
        "if remainder > 0:\n",
        "    for j in range(remainder):\n",
        "        col = numerical_columns[num_pairs*6 + j]\n",
        "        sns.distplot(bike_df[col], ax=axes[num_pairs, j])\n",
        "        axes[num_pairs, j].set_xlabel(col)\n",
        "        axes[num_pairs, j].set_ylabel('Density')\n",
        "        axes[num_pairs, j].set_title(f'Distribution Plot: {col}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v_SyDgj-_bxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "categorical_features = bike_df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "num_plots = len(categorical_features)\n",
        "num_cols = 3\n",
        "num_rows = (num_plots + num_cols - 1) // num_cols\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(25, 5*num_rows))\n",
        "fig.tight_layout()\n",
        "\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    row = i // num_cols\n",
        "    col = i % num_cols\n",
        "\n",
        "    ax = axes[row, col]\n",
        "    sns.boxplot(data=bike_df, x=feature, y='Rented_Bike_Count', ax=ax)\n",
        "    ax.set_xlabel(feature)\n",
        "    ax.set_ylabel('Rented Bike Count')\n",
        "    ax.set_title(f'Rented Bike Count by {feature}', loc='left')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NYO0-mG0EbFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "x = bike_df['Temperature']\n",
        "y = bike_df['Humidity']\n",
        "z = bike_df['Rented_Bike_Count']\n",
        "\n",
        "fig = go.Figure(data=[go.Scatter3d(\n",
        "    x=x,\n",
        "    y=y,\n",
        "    z=z,\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=5,\n",
        "        color=z,\n",
        "        colorscale='Viridis',\n",
        "        opacity=0.8\n",
        "    )\n",
        ")])\n",
        "\n",
        "fig.update_layout(\n",
        "    scene=dict(\n",
        "        xaxis_title='Temperature',\n",
        "        yaxis_title='Humidity',\n",
        "        zaxis_title='Rented Bike Count'\n",
        "    ),\n",
        "    title='3D Scatter Plot of Bike Data',\n",
        "    width=800,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "w6_wc6uyI7se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Create a copy of the original DataFrame with the specified columns\n",
        "cat_columns = ['Hour', 'Seasons', 'Holiday', 'Functioning_Day', 'month', 'weekdays_weekend']\n",
        "cat_data = bike_df[cat_columns + ['Rented_Bike_Count']].copy()\n",
        "\n",
        "# Apply scaling to the 'Rented Bike Count' column\n",
        "scaler = MinMaxScaler()\n",
        "cat_data['Rented_Bike_Count'] = scaler.fit_transform(cat_data['Rented_Bike_Count'].values.reshape(-1, 1))\n",
        "\n",
        "# Add a 'Count' column to count the occurrences of each category\n",
        "cat_data['Count'] = 1\n",
        "\n",
        "# Group the data by the categorical columns and count the occurrences over time\n",
        "grouped_data = cat_data.groupby(cat_columns).sum().reset_index()\n",
        "\n",
        "# Plot the line plot\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "sns.lineplot(data=grouped_data, x='Hour', y='Rented_Bike_Count', hue='Seasons', style='weekdays_weekend', markers=True, ax=ax)\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_xlabel('Hour')\n",
        "ax.set_ylabel('Scaled Rented Bike Count')\n",
        "ax.set_title('Scaled Rented Bike Count of Categorical Variables over Time')\n",
        "ax.legend(title='Seasons')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nxK1tm8YYQ3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Select the numerical features\n",
        "numerical_features = ['Temperature', 'Humidity', 'Wind_speed', 'Visibility', 'Dew_point_temperature',\n",
        "                      'Solar_Radiation', 'Rainfall', 'Snowfall', 'Rented_Bike_Count']\n",
        "\n",
        "# Copy the selected columns from the DataFrame\n",
        "num_data = bike_df[numerical_features].copy()\n",
        "\n",
        "# Apply scaling to the numerical features\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(num_data.values)\n",
        "num_data_scaled = pd.DataFrame(scaled_data, columns=num_data.columns)\n",
        "\n",
        "# Select the features to include in the line plot\n",
        "selected_features = ['Temperature', 'Humidity', 'Wind_speed','Snowfall', 'Rainfall']\n",
        "\n",
        "# Plot the line plot\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "for feature in selected_features:\n",
        "    sns.lineplot(data=num_data_scaled, x=feature, y='Rented_Bike_Count', ax=ax, label=feature)\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_xlabel('Scaled Numerical Features')\n",
        "ax.set_ylabel('Scaled Rented Bike Count')\n",
        "ax.set_title('Scaled Rented Bike Count vs. Scaled Numerical Features')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0XBw7bx_gz_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.corr()['Rented_Bike_Count']"
      ],
      "metadata": {
        "id": "LZ4ptUJns4Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "## plot the Correlation matrix\n",
        "plt.figure(figsize=(20,8))\n",
        "correlation=bike_df.corr()\n",
        "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
        "sns.heatmap((correlation),mask=mask, annot=True,cmap='coolwarm')"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(bike_df,hue='Seasons',corner=True)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assumption : Observations are identically distributed(all the elemets in the test have equail probability to ocure)"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cheking Histogram\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.hist(bike_df['Rented_Bike_Count'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YfeIXZNE-vcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Help from Python\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "DataToTest = bike_df['Rented_Bike_Count']\n",
        "\n",
        "stat, p = shapiro(DataToTest)\n",
        "\n",
        "print('stat=%.2f, p=%.30f' % (stat, p))\n",
        "\n",
        "if p > 0.05:\n",
        "    print('Normal distribution')\n",
        "else:\n",
        "    print('Not a normal distribution')"
      ],
      "metadata": {
        "id": "3wFvj177AtNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normality test using Shapiro-Wilk Test : tests If data is normally distributed"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Shapiro-Wilk test is used to assess the normality of a dataset. It helps determine whether the data follows a normal distribution or deviates significantly from it."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asumption - Identical and Normal Distribution"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.corr()['Rented_Bike_Count']"
      ],
      "metadata": {
        "id": "uvpmAN-nF-Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FirstSample =bike_df[1:30]['Rented_Bike_Count']\n",
        "SecondSample = bike_df[1:30]['Temperature']\n",
        "\n",
        "plt.plot(FirstSample,SecondSample)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GeP0yyFcFu6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spearman Rank Correlation\n",
        "from scipy.stats import spearmanr\n",
        "stat, p = spearmanr(FirstSample, SecondSample)\n",
        "\n",
        "print('stat=%.3f, p=%5f' % (stat, p))\n",
        "if p > 0.05:\n",
        "    print('independent samples')\n",
        "else:\n",
        "    print('dependent samples')"
      ],
      "metadata": {
        "id": "8bSN0gsGGRFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pearson correlation\n",
        "from scipy.stats import pearsonr\n",
        "stat, p = pearsonr(FirstSample, SecondSample)\n",
        "\n",
        "print('stat=%.3f, p=%5f' % (stat, p))\n",
        "if p > 0.05:\n",
        "    print('independent samples')\n",
        "else:\n",
        "    print('dependent samples')"
      ],
      "metadata": {
        "id": "VJn_-wBYGr0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "te7_e6AaGlT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer.\n",
        "Correlation Test - Pearson and Spearman’s Rank Correlation"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson correlation is used to measure the strength and direction of a linear relationship between two continuous variables.\n",
        "\n",
        "Spearman's rank correlation is used to measure the strength and direction of a monotonic relationship between two variables, which can be continuous or ranked."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contingency_data = pd.crosstab(bike_df['Seasons'], bike_df['Holiday'],margins = False)\n",
        "contingency_data"
      ],
      "metadata": {
        "id": "d-d5G-0CJYlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "import scipy.stats as stats\n",
        "\n",
        "\n",
        "# Perform the chi-square test\n",
        "chi2, p_value, dof, expected = stats.chi2_contingency(contingency_data)\n",
        "\n",
        "# Print the chi-square test result\n",
        "print(\"Chi-square statistic:\", chi2)\n",
        "print(\"p-value:\", p_value)\n",
        "print(\"Degrees of freedom:\", dof)\n",
        "\n",
        "if p > 0.05:\n",
        "    print('independent categories')\n",
        "else:\n",
        "    print('dependent categories')"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the chi-square test results, the chi-square statistic is 122.59, the p-value is approximately 2.14e-26, and the degrees of freedom is 3.\n",
        "\n",
        "Since the p-value is extremely small (smaller than the typical significance level of 0.05), we can reject the null hypothesis of independence between the \"Seasons\" and \"Holiday\" variables. This suggests that there is a significant association or dependency between the two variables in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the occurrence of seasons and holidays in the dataset is not independent of each other. The variables \"Seasons\" and \"Holiday\" are related, and the difference in their frequencies is statistically significant."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install missingno -q"
      ],
      "metadata": {
        "id": "-6Lq2vYAPPiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import missingno as msno\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame named bike_df\n",
        "msno.matrix(bike_df)\n"
      ],
      "metadata": {
        "id": "0hRF3Fp5PMAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "print(bike_df.isna().sum())"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lockely there are no missing values in the dataset.so i don't use any missing value imputation technique"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rented_Bike_Count is skewed so i use boxplot"
      ],
      "metadata": {
        "id": "TzdxyCb4WDAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.skew()"
      ],
      "metadata": {
        "id": "9NSvaBAdXUdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['Rented_Bike_Count'].skew()"
      ],
      "metadata": {
        "id": "gxwhguzVXISQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "#Boxplot of Rented Bike Count to check outliers\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "sns.boxplot(x=bike_df['Rented_Bike_Count'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above boxplot shows that we have detect outliers in Rented Bike Count column"
      ],
      "metadata": {
        "id": "dnMdxaLOMdNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "ax = sns.distplot(np.sqrt(bike_df['Rented_Bike_Count']), color=\"purple\")\n",
        "ax.axvline(np.sqrt(bike_df['Rented_Bike_Count']).mean(), color='blue', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(np.sqrt(bike_df['Rented_Bike_Count']).median(), color='green', linestyle='dashed', linewidth=2)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e7s97FLUQOn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#After applying sqrt on Rented Bike Count check wheater we still have outliers\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "sns.boxplot(x=np.sqrt(bike_df['Rented_Bike_Count']))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I06GwwIpQZaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['Rented_Bike_Count']=np.sqrt(bike_df['Rented_Bike_Count'])"
      ],
      "metadata": {
        "id": "61osj4CJpKC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.sqrt(bike_df['Rented_Bike_Count']) will return a new Series object with the square root of each value in the 'Rented_Bike_Count' column.\n",
        "\n",
        "\n",
        "This transformation is often applied to data to reduce skewness, as taking the square root can help normalize the distribution and make it more symmetric.\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign all catagoriacla features to a variable\n",
        "categorical_features = bike_df.select_dtypes(include=['object', 'category']).columns\n",
        "categorical_features"
      ],
      "metadata": {
        "id": "JYhjt2eSvGJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using one hot encoding for domification\n",
        "bike_df_copy = pd.get_dummies(bike_df, columns=categorical_features, drop_first=True)"
      ],
      "metadata": {
        "id": "1Sa6z0tHwcjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding is a common preprocessing step for categorical features in machine learning models. By converting categorical variables into binary columns, it allows machine learning algorithms to work with categorical data more effectively."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "bike_df_copy.columns"
      ],
      "metadata": {
        "id": "Ouq3k8We9kXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = bike_df_copy.drop(columns=['Rented_Bike_Count'], axis=1)\n",
        "y = np.sqrt(bike_df_copy['Rented_Bike_Count'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=0)\n"
      ],
      "metadata": {
        "id": "TfqK-7xC9kG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with the following function we can select highly correlated features\n",
        "# it will remove the first feature that is correlated with anything other feature\n",
        "\n",
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()  # Set of all the names of correlated columns\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if (corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
        "                colname = corr_matrix.columns[i]  # getting the name of column\n",
        "                col_corr.add(colname)\n",
        "    return col_corr"
      ],
      "metadata": {
        "id": "_jIwgk7JBAOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_features = correlation(X_train, 0.7)\n",
        "len(set(corr_features))"
      ],
      "metadata": {
        "id": "4p48O1okBO4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_features"
      ],
      "metadata": {
        "id": "6rCqZ49LBUpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "X_train.drop(corr_features,axis=1)\n",
        "X_test.drop(corr_features,axis=1)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df_copy=bike_df_copy.drop('Dew_point_temperature', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "JetEAE3rQl_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlated features are features that exhibit a high degree of linear relationship or dependency between them. Having highly correlated features can introduce multicollinearity in a model, which can affect the model's performance and interpretability.\n",
        "\n",
        "Correlated features are features that exhibit a high degree of linear relationship or dependency between them. Having highly correlated features can introduce multicollinearity in a model, which can affect the model's performance and interpretability."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "other featuers are not correlated to each other, Correlated features are not considered important because they can introduce redundancy, affect model interpretability, lead to model instability, and increase the risk of overfitting. Selecting uncorrelated features can improve model performance, enhance interpretability, and ensure more reliable and generalized results."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6-CH1ZPkuc-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the distplots without any transformation\n",
        "\n",
        "for col in X_train.columns:\n",
        "    plt.figure(figsize=(8,2))\n",
        "    plt.subplot(121)\n",
        "    sns.distplot(X_train[col])\n",
        "    plt.title(col)\n",
        "\n",
        "    plt.subplot(122)\n",
        "    stats.probplot(X_train[col], dist=\"norm\", plot=plt)\n",
        "    plt.title(col)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "A7cV0qXslB7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Apply Yeo-Johnson transform\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "import pandas as pd\n",
        "\n",
        "pt1 = PowerTransformer()\n",
        "X_train_transformed2 = pt1.fit_transform(X_train)\n",
        "X_test_transformed2 = pt1.transform(X_test)\n",
        "\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Perform cross-validation with 5 folds\n",
        "cv_scores = cross_val_score(lr, X_train_transformed2, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# Train the model on the full training set\n",
        "lr.fit(X_train_transformed2, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred3 = lr.predict(X_test_transformed2)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred3)\n",
        "print(\"R-squared score:\", r2)\n",
        "\n",
        "df = pd.DataFrame({'cols': X_train.columns, 'Yeo_Johnson_lambdas': pt1.lambdas_})\n",
        "# print(df)\n",
        "\n",
        "print(\"Cross-validated R-squared scores:\", cv_scores)\n",
        "print(\"Mean R-squared score:\", cv_scores.mean())\n",
        "mean_r2 = np.mean(cv_scores)\n",
        "print(\"Mean R-squared score:\", mean_r2)\n"
      ],
      "metadata": {
        "id": "rCk44k3QOqIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_transformed = pd.DataFrame(X_train_transformed2,columns=X_train.columns)\n",
        "\n",
        "for col in X_train_transformed.columns:\n",
        "    plt.figure(figsize=(8,2))\n",
        "    plt.subplot(121)\n",
        "    sns.distplot(X_train[col])\n",
        "    plt.title(col)\n",
        "\n",
        "    plt.subplot(122)\n",
        "    sns.distplot(X_train_transformed[col])\n",
        "    plt.title(col)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BTkrb60jmEsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit the scaler to the train set, it will learn the parameters\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# transform train and test sets\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Instantiate a LinearRegression model\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Fit the model using the scaled features (X_train_scaled) and the continuous labels (y_train)\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = lr.predict(X_test_scaled)\n",
        "\n",
        "# Calculate the mean squared error (MSE) on the test set\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Assuming y_test contains the true target values and y_pred contains the predicted values\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared score:\", r2)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Assuming X_train_scaled and y_train contain the scaled training data and target values, respectively\n",
        "# Assuming lr is your trained linear regression model\n",
        "\n",
        "# Perform cross-validation with 5 folds\n",
        "cv_scores = cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# Print the cross-validated R-squared scores\n",
        "print(\"Cross-validated R-squared scores:\", cv_scores)\n",
        "print(\"Mean R-squared score:\", cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
      ],
      "metadata": {
        "id": "6RwKqMsZCZG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
        "\n",
        "ax1.scatter(X_train['Visibility'], X_train['Temperature'])\n",
        "ax1.set_title(\"Before Scaling\")\n",
        "ax2.scatter(X_train_scaled['Visibility'], X_train_scaled['Temperature'],color='red')\n",
        "ax2.set_title(\"After Scaling\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fdUiw4bxCurQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
        "\n",
        "# before scaling\n",
        "ax1.set_title('Before Scaling')\n",
        "sns.kdeplot(X_train['Visibility'], ax=ax1)\n",
        "sns.kdeplot(X_train['Temperature'], ax=ax1)\n",
        "\n",
        "# after scaling\n",
        "ax2.set_title('After Standard Scaling')\n",
        "sns.kdeplot(X_train_scaled['Visibility'], ax=ax2)\n",
        "sns.kdeplot(X_train_scaled['Temperature'], ax=ax2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8rwiREX6EiXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison of Distributions"
      ],
      "metadata": {
        "id": "T9_Gavx9E1wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
        "\n",
        "# before scaling\n",
        "ax1.set_title('Age Distribution Before Scaling')\n",
        "sns.kdeplot(X_train['Temperature'], ax=ax1)\n",
        "\n",
        "# after scaling\n",
        "ax2.set_title('Age Distribution After Standard Scaling')\n",
        "sns.kdeplot(X_train_scaled['Temperature'], ax=ax2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MTSqNjt_E36w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "MinMaxScaler_scaler = MinMaxScaler()\n",
        "\n",
        "# fit the scaler to the train set, it will learn the parameters\n",
        "MinMaxScaler_scaler.fit(X_train)\n",
        "\n",
        "# transform train and test sets\n",
        "X_train_scaled_MinMaxScaler = MinMaxScaler_scaler.transform(X_train)\n",
        "X_test_scaled_MinMaxScaler = MinMaxScaler_scaler.transform(X_test)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create an instance of the LinearRegression model\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Fit the model on the scaled training data\n",
        "lr.fit(X_train_scaled_MinMaxScaler, y_train)\n",
        "\n",
        "# Predict the target variable for the scaled test data\n",
        "y_pred_MinMaxScaler = lr.predict(X_test_scaled_MinMaxScaler)\n",
        "\n",
        "# Calculate the mean squared error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred_MinMaxScaler)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Calculate the R-squared score\n",
        "r2 = r2_score(y_test, y_pred_MinMaxScaler)\n",
        "print(\"R-squared score:\", r2)\n",
        "\n",
        "# Perform cross-validation with 5 folds\n",
        "cv_scores_MinMaxScaler = cross_val_score(lr, X_train_scaled_MinMaxScaler, y_train, cv=5, scoring='r2')\n",
        "mean_r2_MinMaxScaler = np.mean(cv_scores_MinMaxScaler)\n",
        "print(\"Mean cross-validated R-squared score:\", mean_r2_MinMaxScaler)\n",
        "print(\"Mean R-squared score:\", cv_scores_MinMaxScaler.mean())\n"
      ],
      "metadata": {
        "id": "SXDrbpb8Jkh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled_MinMaxScaler = pd.DataFrame(X_train_scaled_MinMaxScaler, columns=X_train.columns)\n",
        "X_test_scaled_MinMaxScaler = pd.DataFrame(X_test_scaled_MinMaxScaler, columns=X_test.columns)"
      ],
      "metadata": {
        "id": "KRwRk7CNIltU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
        "\n",
        "ax1.scatter(X_train['Visibility'], X_train['Temperature'])\n",
        "ax1.set_title(\"Before Scaling\")\n",
        "ax2.scatter(X_train_scaled_MinMaxScaler['Visibility'], X_train_scaled_MinMaxScaler['Temperature'],color='red')\n",
        "ax2.set_title(\"After Scaling\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U5VAOz6Pcy7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
        "\n",
        "# before scaling\n",
        "ax1.set_title('Before Scaling')\n",
        "sns.kdeplot(X_train['Visibility'], ax=ax1)\n",
        "sns.kdeplot(X_train['Temperature'], ax=ax1)\n",
        "\n",
        "# after scaling\n",
        "ax2.set_title('After Standard Scaling')\n",
        "sns.kdeplot(X_train_scaled_MinMaxScaler['Visibility'], ax=ax2)\n",
        "sns.kdeplot(X_train_scaled_MinMaxScaler['Temperature'], ax=ax2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VXVKIXWEde-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
        "\n",
        "# before scaling\n",
        "ax1.set_title('Age Distribution Before Scaling')\n",
        "sns.kdeplot(X_train['Temperature'], ax=ax1)\n",
        "\n",
        "# after scaling\n",
        "ax2.set_title('Age Distribution After Standard Scaling')\n",
        "sns.kdeplot(X_train_scaled_MinMaxScaler['Temperature'], ax=ax2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o8dpSwR9dzCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why scaling is important?"
      ],
      "metadata": {
        "id": "66hsd-K1FH25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling is important in machine learning to:\n",
        "- Normalize the features and bring them to a comparable range.\n",
        "- Improve algorithm convergence and stability.\n",
        "- Prevent numerical instability and biased results.\n",
        "- Ensure compatibility with certain algorithms.\n",
        "- Enable fair and accurate feature comparisons."
      ],
      "metadata": {
        "id": "5i6Yxa5ezvue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use standarization method just buecause this parform slighty better then normalization mathod"
      ],
      "metadata": {
        "id": "kAh9MR3YeJ1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Curse of Dimensionality\" refers to the challenges that arise when dealing with high-dimensional data. It causes sparsity of data, increased computational complexity, overfitting, and difficulties with distance-based algorithms. Mitigation strategies include feature selection, dimensionality reduction, regularization, and leveraging domain knowledge."
      ],
      "metadata": {
        "id": "5154b1QEhqvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply PCA to the training set:"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n"
      ],
      "metadata": {
        "id": "jH_0qkOz6Xi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,47):\n",
        "  pca = PCA(n_components=i)  # Specify the number of components to keep\n",
        "  X_train_pca = pca.fit_transform(X_train)\n",
        "#Fit a linear regression model on the transformed training set:\n",
        "  lrp = LinearRegression()\n",
        "  lrp.fit(X_train_pca, y_train)\n",
        "# Transform the test set using the same PCA transformation:\n",
        "  X_test_pca = pca.transform(X_test)\n",
        "# Make predictions on the transformed test set:\n",
        "  y_pred = lrp.predict(X_test_pca)\n",
        "# Evaluate the performance of the model using the R-squared score:\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  print(\"R-squared score:\", r2)"
      ],
      "metadata": {
        "id": "dEThsA-A7cqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=45)  # Specify the number of components to keep\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "#Fit a linear regression model on the transformed training set:\n",
        "lrp = LinearRegression()\n",
        "lrp.fit(X_train_pca, y_train)\n",
        "# Transform the test set using the same PCA transformation:\n",
        "X_test_pca = pca.transform(X_test)\n",
        "# Make predictions on the transformed test set:\n",
        "y_pred = lrp.predict(X_test_pca)\n",
        "# Evaluate the performance of the model using the R-squared score:\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared score:\", r2)\n"
      ],
      "metadata": {
        "id": "ag0oju0VuBuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforming in 3D\n",
        "pca = PCA(n_components=3)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "metadata": {
        "id": "5jGDu0l4-LHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "y_train_pca = y_train.astype(str)\n",
        "fig = px.scatter_3d(df, x=X_train_pca[:,0], y=X_train_pca[:,1], z=X_train_pca[:,2],\n",
        "              color=y_train_pca)\n",
        "fig.update_layout(\n",
        "    margin=dict(l=20, r=20, t=20, b=20),\n",
        "    paper_bgcolor=\"LightSteelBlue\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "MW7T8_Q5-eid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))"
      ],
      "metadata": {
        "id": "7ydmr7qT85WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA is appled on dataset. number of columns are not very high. so PCA dont help much.\n",
        "\n",
        "Dimensionality reduction techniques are used to reduce the number of features or variables in a dataset while preserving the essential information. These techniques are beneficial when working with high-dimensional data, as they can help in simplifying the data representation, reducing computational complexity, and removing noise or redundant information.\n",
        "\n",
        "\n",
        "\n",
        "Principal Component Analysis (PCA): PCA is a popular linear technique that transforms the original variables into a new set of uncorrelated variables called principal components. These components are ordered by the amount of variance they explain in the data, allowing for dimensionality reduction by selecting a subset of the components that capture most of the variability."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = bike_df_copy.drop(columns=['Rented_Bike_Count'], axis=1)\n",
        "y = np.sqrt(bike_df_copy['Rented_Bike_Count'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=0)"
      ],
      "metadata": {
        "id": "3O7QDsHZ9LVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "mPzCYBQTp1nV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A data splitting ratio of 75% for training and 25% for testing was used. This ratio is commonly used in machine learning and is known as the 75/25 split.\n",
        "\n",
        "The reason for choosing this ratio is to strike a balance between having enough data for training the model and having enough data for evaluating its performance. By allocating 75% of the data for training, the model has a substantial amount of data to learn from and generalize patterns. The remaining 25% is used for testing, allowing us to assess how well the model performs on unseen data and evaluate its generalization capabilities."
      ],
      "metadata": {
        "id": "YUj2iIrVpV8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Rented_Bike_Count' column is left skewed in dataset, but in privous outlier removing process i apply np.squr and is column convarted into normal distribution.. so now this dataset is balanced."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "#Cheking Histogram\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.hist(bike_df['Rented_Bike_Count'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.ylabel('Density')\n",
        "ax = sns.distplot(bike_df['Rented_Bike_Count'], color=\"green\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5bx0K7oDnpE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.sqrt(bike_df['Rented_Bike_Count']) will return a new Series object with the square root of each value in the 'Rented_Bike_Count' column.\n",
        "\n",
        "\n",
        "This transformation is often applied to data to reduce skewness, as taking the square root can help normalize the distribution and make it more symmetric."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "Hqe6HguwPGsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Fit the Algorithm\n",
        "reg= LinearRegression()\n",
        "reg.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "reg.score(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "t5lPz2SrYlwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "#get the X_train and X-test value\n",
        "y_pred_train=reg.predict(X_train)\n",
        "y_pred_test=reg.predict(X_test)\n",
        "print(\"Predicted target values:\", y_pred_test)"
      ],
      "metadata": {
        "id": "3h8BHtLTYbVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculate the evaluation metric scores\n",
        "train_score = r2_score(y_train, y_pred_train)\n",
        "print(train_score)\n",
        "test_score = r2_score(y_test, y_pred_test)\n",
        "print(test_score)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross checking with cross val score\n",
        "np.mean(cross_val_score(reg,X_train, y_train,scoring='r2',cv=10))"
      ],
      "metadata": {
        "id": "JqBosqDxR9Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STbYFr4xcs3u"
      },
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error((y_train), (y_pred_train))\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_train, y_pred_train)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score(y_train, y_pred_train)\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WnqF_7IWo1pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define the parameter grid for GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'fit_intercept': [True, False],\n",
        "}\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Perform GridSearchCV with cross-validation\n",
        "grid_search = GridSearchCV(reg, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Print the best parameters and best score\n",
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "print(\"Best Score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "gdiUKBpapzs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Perform cross-validation with the best estimator\n",
        "linear_regression_best = grid_search.best_estimator_\n",
        "cv_scores = cross_val_score(linear_regression_best, X_train, y_train, cv=10)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Mean CV Score:\", cv_scores.mean())\n",
        "\n",
        "# Fit the best estimator on the training data\n",
        "linear_regression_best.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "test_score = linear_regression_best.score(X_test, y_test)\n",
        "print(\"Test Score:\", test_score)"
      ],
      "metadata": {
        "id": "pkBp5w-hQoa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "GridSearchCV is a class in scikit-learn that facilitates performing an exhaustive search over specified parameter values for an estimator. It is commonly used for hyperparameter tuning, which involves finding the optimal values for the hyperparameters of a machine learning model."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yse, there are slight improvement in the model.\n",
        "\n",
        "\n",
        "\n",
        "without girdsearchcv--\n",
        "\n",
        "test Score:  0.7688887774277025\n",
        "\n",
        "\n",
        "with girdsearchcv--\n",
        "\n",
        "\n",
        "\n",
        "Test Score: 0.7905536900393838\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DECISION TREE**"
      ],
      "metadata": {
        "id": "tCdtO__zTzpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "decision_regressor = DecisionTreeRegressor(max_depth=None,\n",
        "                                           max_features=12,\n",
        "                                           max_leaf_nodes=150)\n",
        "\n",
        "# Fit the Algorithm\n",
        "decision_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "HLjCjUesT_Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "#get the X_train and X-test value\n",
        "y_pred_train_d = decision_regressor.predict(X_train)\n",
        "y_pred_test_d = decision_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "bNUiYRWoUV9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have defined and assigned values to X_train and y_train\n",
        "\n",
        "# Calculate the R² score on the training data\n",
        "score_train = decision_regressor.score(X_train, y_train)\n",
        "\n",
        "# Print the R² score\n",
        "print(\"R² score on training data:\", score_train)\n"
      ],
      "metadata": {
        "id": "t7LDKyvRcfC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",decision_regressor.score(X_train,y_train))\n",
        "\n",
        "#calculate MSE\n",
        "MSE_d= mean_squared_error(y_train, y_pred_train_d)\n",
        "print(\"MSE :\",MSE_d)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_d=np.sqrt(MSE_d)\n",
        "print(\"RMSE :\",RMSE_d)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_d= mean_absolute_error(y_train, y_pred_train_d)\n",
        "print(\"MAE :\",MAE_d)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_d= r2_score(y_train, y_pred_train_d)\n",
        "print(\"R2 :\",r2_d)\n",
        "Adjusted_R2_d=(1-(1-r2_score(y_train, y_pred_train_d))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_d))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [None, 5, 8, 9, 10, 15, 16],\n",
        "    'max_features': [2, 5, 9, 10, 12],\n",
        "    'max_leaf_nodes': [100, 150, 180]\n",
        "}\n",
        "\n",
        "# Create an instance of DecisionTreeRegressor\n",
        "decision_regressor = DecisionTreeRegressor()\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(decision_regressor, param_grid, cv=5)\n",
        "\n",
        "# Fit the data to perform the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Use the best estimator for predictions\n",
        "best_estimator = grid_search.best_estimator_\n",
        "y_pred_test_tuned = best_estimator.predict(X_test)\n"
      ],
      "metadata": {
        "id": "VLHzOhSzScf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best parameters and best score\n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best Score: \", best_score)\n",
        "\n"
      ],
      "metadata": {
        "id": "i7v-3vI-su_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform cross-validation with best parameters\n",
        "cross_val_scores = cross_val_score(decision_regressor,X_train, y_train, cv=10)\n",
        "print(\"Cross-Validation Scores: \", cross_val_scores)\n",
        "print(\"Average Cross-Validation Score: \", cross_val_scores.mean())"
      ],
      "metadata": {
        "id": "33M5zA0_s81J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "GridSearchCV is a class in scikit-learn that facilitates performing an exhaustive search over specified parameter values for an estimator. It is commonly used for hyperparameter tuning, which involves finding the optimal values for the hyperparameters of a machine learning model."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "no i dont seen any improvement.\n",
        "\n",
        "\n",
        "\n",
        "without girdsearchcv--\n",
        "\n",
        "test Score: 0.8512091074696333\n",
        "\n",
        "with girdsearchcv--\n",
        "\n",
        "\n",
        "\n",
        "Test Score: 0.7410361966835403"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each evaluation metric provides a different perspective on the performance of a machine learning model. Here's an explanation of each metric and its indication towards the business impact of the ML model used:\n",
        "\n",
        "1. Mean Squared Error (MSE):\n",
        "   - MSE measures the average squared difference between the predicted and actual values.\n",
        "   - It gives more weight to larger errors due to the squaring operation.\n",
        "   - A lower MSE indicates better model performance and suggests that the model's predictions are closer to the actual values.\n",
        "   - In terms of business impact, a lower MSE implies that the model's predictions have less error and are more accurate. This can lead to more reliable decision-making and improved efficiency in business operations.\n",
        "\n",
        "2. Root Mean Squared Error (RMSE):\n",
        "   - RMSE is the square root of MSE and represents the average magnitude of the prediction errors.\n",
        "   - It is expressed in the same units as the target variable, making it easily interpretable.\n",
        "   - Like MSE, a lower RMSE indicates better model performance and suggests that the model's predictions are closer to the actual values.\n",
        "   - From a business perspective, a lower RMSE implies that the model's predictions have smaller errors on average. This can increase trust in the model's output and support decision-making with more confidence.\n",
        "\n",
        "3. Mean Absolute Error (MAE):\n",
        "   - MAE measures the average absolute difference between the predicted and actual values.\n",
        "   - Unlike MSE, it does not square the errors, giving equal weight to all errors.\n",
        "   - A lower MAE indicates better model performance and suggests that the model's predictions have less absolute error.\n",
        "   - In terms of business impact, a lower MAE means that the model's predictions are closer to the actual values on average. This can lead to better resource allocation, improved planning, and reduced costs.\n",
        "\n",
        "4. R-squared (R2):\n",
        "   - R2 measures the proportion of the variance in the target variable that is explained by the model.\n",
        "   - It ranges from 0 to 1, with higher values indicating a better fit of the model to the data.\n",
        "   - An R2 of 1 means that the model explains all the variability in the target variable.\n",
        "   - From a business perspective, a higher R2 suggests that the model captures a larger portion of the target variable's variation, indicating its ability to provide useful insights and predictions.\n",
        "\n",
        "5. Adjusted R-squared (Adjusted R2):\n",
        "   - Adjusted R2 is a modified version of R2 that adjusts for the number of predictors in the model.\n",
        "   - It penalizes the addition of irrelevant predictors that do not improve the model's performance significantly.\n",
        "   - The adjusted R2 value can be lower than R2 if the model's added predictors are not informative.\n",
        "   - In terms of business impact, the adjusted R2 helps assess the incremental value of adding more predictors to the model. It provides a more conservative estimate of the model's explanatory power and helps in avoiding overfitting.\n",
        "\n",
        "The business impact of the ML model used depends on the specific context and objectives of the business. However, in general, a well-performing ML model with lower MSE, RMSE, and MAE indicates more accurate predictions and improved decision-making. Higher R2 and adjusted R2 values suggest that the model captures more of the target variable's variation, enabling valuable insights and predictions for the business. Ultimately, a reliable and high-performing ML model can lead to enhanced efficiency, cost reduction, improved resource allocation, and better-informed business decisions."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RandomForestRegressor"
      ],
      "metadata": {
        "id": "nsGqMgn6W2Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "#import the packages\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Create an instance of the RandomForestRegressor\n",
        "rf_model = RandomForestRegressor()"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "rf_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "AZ_Cb631Ly4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_train_r = rf_model.predict(X_train)\n",
        "y_pred_test_r = rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "Wdn4UAa5L9Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "Ka5kgIgux17A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",rf_model.score(X_train,y_train))\n",
        "\n",
        "#calculate MSE\n",
        "MSE_rf= mean_squared_error(y_train, y_pred_train_r)\n",
        "print(\"MSE :\",MSE_rf)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_rf=np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\",RMSE_rf)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_rf= mean_absolute_error(y_train, y_pred_train_r)\n",
        "print(\"MAE :\",MAE_rf)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_rf= r2_score(y_train, y_pred_train_r)\n",
        "print(\"R2 :\",r2_rf)\n",
        "Adjusted_R2_rf=(1-(1-r2_score(y_train, y_pred_train_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Number of trees in random forest\n",
        "n_estimators = [20,60,100,120]\n",
        "\n",
        "# Number of features to consider at every split\n",
        "max_features = [0.2,0.6,1.0]\n",
        "\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [2,8,None]\n",
        "\n",
        "# Number of samples\n",
        "max_samples = [0.5,0.75,1.0]\n",
        "\n",
        "# Bootstrap samples\n",
        "bootstrap = [True,False]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2]\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "              'max_samples':max_samples,\n",
        "              'bootstrap':bootstrap,\n",
        "              'min_samples_split':min_samples_split,\n",
        "              'min_samples_leaf':min_samples_leaf\n",
        "             }\n",
        "print(param_grid)"
      ],
      "metadata": {
        "id": "IS6-ixkYZUP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "rf_grid = RandomizedSearchCV(estimator = rf_model,\n",
        "                       param_distributions = param_grid,\n",
        "                       cv = 5,\n",
        "                       verbose=2,\n",
        "                       n_jobs = -1)"
      ],
      "metadata": {
        "id": "75K8xqpAZY77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_grid.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "1Oj-GIEmZbRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_grid.best_params_"
      ],
      "metadata": {
        "id": "jPKOw22MZdio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_grid.best_score_"
      ],
      "metadata": {
        "id": "XEe0mAz4a2Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is a class in scikit-learn that allows you to perform an exhaustive search over a specified parameter grid to find the best combination of hyperparameters for a given model. It automates the process of tuning hyperparameters by evaluating the model's performance on different combinations of parameter values using cross-validation.\n",
        "\n",
        "Using GridSearchCV helps automate the process of hyperparameter tuning and allows you to find the optimal set of hyperparameters for your model based on the specified parameter grid and evaluation metric.\n",
        "\n",
        "GridSearchCV is chosen for hyperparameter tuning because it performs an exhaustive search over all specified combinations of hyperparameters, incorporates cross-validation for robust evaluation, automates the process, identifies optimal hyperparameters, and improves the generalizability of the model.\n",
        "\n",
        "\n",
        "GridSearchCV is good for small dataset. this take more time and more comutation in camparistion to randonmisesearchcv. if we working on big dataset then we chouse other approch to reduse time.\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "without randomsearchcv\n",
        "\n",
        "\n",
        "\n",
        "best score : 0.9853788678117673\n",
        "\n",
        "\n",
        "with randomsearchcv\n",
        "\n",
        "\n",
        "beat soore : 0.8804336666598515\n",
        "\n",
        "\n",
        "there are no improvenmat to apply hyparparameter tuning..\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When considering a Random Forest model for a positive business impact, the following evaluation metrics can be considered:\n",
        "\n",
        "1. Mean Squared Error (MSE) or Root Mean Squared Error (RMSE):\n",
        "   - These metrics measure the average squared or square root of the differences between the predicted and actual values.\n",
        "   - Lower MSE or RMSE values indicate better model performance with smaller prediction errors.\n",
        "   - In a business context, lower MSE or RMSE suggests more accurate predictions, which can lead to improved decision-making, cost reduction, and increased operational efficiency.\n",
        "\n",
        "2. Mean Absolute Error (MAE):\n",
        "   - MAE measures the average absolute difference between the predicted and actual values.\n",
        "   - Similar to MSE and RMSE, lower MAE values indicate better model performance with smaller errors.\n",
        "   - In a business setting, lower MAE suggests more accurate predictions, which can support decision-making, resource allocation, and cost reduction.\n",
        "\n",
        "3. R-squared (R2) or Adjusted R-squared (Adjusted R2):\n",
        "   - R2 measures the proportion of the variance in the target variable explained by the model.\n",
        "   - Higher R2 or Adjusted R2 values indicate a better fit of the model to the data.\n",
        "   - In a business context, higher R2 or Adjusted R2 values suggest that the model captures a larger portion of the target variable's variation, providing valuable insights for decision-making and prediction purposes.\n",
        "\n",
        "When evaluating a Random Forest model, it's important to consider multiple metrics to gain a comprehensive understanding of its performance. MSE, RMSE, and MAE focus on prediction errors, while R2 and Adjusted R2 assess the model's explanatory power. By assessing these metrics, businesses can determine the accuracy, reliability, and predictive power of the Random Forest model, ultimately leading to informed decision-making and positive business impact."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i get best score in random forest model so choose random forest model.\n",
        "Random Forest is a popular and effective model for prediction due to its high accuracy, robustness to noise and outliers, ability to handle non-linear relationships, feature importance analysis, scalability, and capability to handle missing data."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jU4RpJY-mp_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i am using random forest model\n"
      ],
      "metadata": {
        "id": "-V2Wy1d2tmQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " https://lime-ml.readthedocs.io/en/latest/lime.html"
      ],
      "metadata": {
        "id": "XWLZ8PEU2vl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instaling lime\n",
        "!pip install lime -q"
      ],
      "metadata": {
        "id": "6ogNGapTt-qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "from lime import lime_tabular"
      ],
      "metadata": {
        "id": "XWv7P252269J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the feature names for LIME\n",
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "# Define the LIME explainer\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=feature_names, mode='regression')\n",
        "\n",
        "# Select a random test instance for explanation\n",
        "instance_idx = np.random.randint(len(X_test))\n",
        "instance = X_test.iloc[instance_idx]\n",
        "true_label = y_test.iloc[instance_idx]\n",
        "\n",
        "# Generate an explanation using LIME\n",
        "num_features = 47  # Number of features to include in the explanation\n",
        "exp = explainer.explain_instance(instance.values, rf_model.predict, num_features=num_features)\n",
        "\n",
        "# Print the true label and the LIME explanation\n",
        "print(\"True Label:\", true_label)\n",
        "print(\"LIME Explanation:\")\n",
        "exp.show_in_notebook()\n"
      ],
      "metadata": {
        "id": "3AW5Z0-e2HDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df=bike_df.drop('Dew_point_temperature', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "X = bike_df.drop(columns=['Rented_Bike_Count'], axis=1)\n",
        "y = bike_df['Rented_Bike_Count']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=0)\n",
        "\n",
        "\n",
        "step1 = ColumnTransformer(\n",
        "    transformers=[('col_tnf', OneHotEncoder(sparse=False, drop='first'), categorical_features)],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "step2 = RandomForestRegressor()\n",
        "pipe = Pipeline([\n",
        "    ('step1', step1),\n",
        "    ('step2', step2)\n",
        "])\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Apply the square root transformation to the target variable for evaluation\n",
        "y_test_sqrt = np.sqrt(y_test)\n",
        "y_pred_sqrt = np.sqrt(y_pred)\n",
        "\n",
        "print('R2 score:', r2_score(y_test_sqrt, y_pred_sqrt))\n",
        "print('MAE:', mean_absolute_error(y_test_sqrt, y_pred_sqrt))\n"
      ],
      "metadata": {
        "id": "hOvg0Yhr42Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "import joblib\n",
        "\n",
        "# Assuming 'pipe' is the trained pipeline model\n",
        "joblib.dump(pipe, 'model_file.pkl')"
      ],
      "metadata": {
        "id": "zmNP2bjml_B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('model_file.pkl')\n"
      ],
      "metadata": {
        "id": "_5xOVTNdwlnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a DataFrame for the unseen data\n",
        "unseen_data = pd.DataFrame({\n",
        "    'Hour': [23],\n",
        "    'Temperature': [3.8],\n",
        "    'Humidity': [83],\n",
        "    'Wind_speed': [1.1],\n",
        "    'Visibility': [390],\n",
        "    'Solar_Radiation': [0.0],\n",
        "    'Rainfall': [0.4],\n",
        "    'Snowfall': [0.0],\n",
        "    'Seasons': ['Autumn'],\n",
        "    'Holiday': ['No Holiday'],\n",
        "    'Functioning_Day': ['Yes'],\n",
        "    'month': [11],\n",
        "    'weekdays_weekend': [1]\n",
        "})\n",
        "\n",
        "# Make predictions on the unseen data\n",
        "predictions = loaded_model.predict(unseen_data)\n",
        "\n",
        "# Display the predictions\n",
        "print(predictions)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cenf7OzLO8Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}